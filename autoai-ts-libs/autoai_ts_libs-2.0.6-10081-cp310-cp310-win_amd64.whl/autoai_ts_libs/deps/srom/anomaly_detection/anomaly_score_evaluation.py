# IBM Confidential Materials
# Licensed Materials - Property of IBM
# IBM Smarter Resources and Operation Management
# (C) Copyright IBM Corp. 2021 All Rights Reserved.
# US Government Users Restricted Rights
#  - Use, duplication or disclosure restricted by
#    GSA ADP Schedule Contract with IBM Corp.


"""
.. module:: anomaly_score_evaluation
   :synopsis: anomaly score evaluation

.. moduleauthor:: SROM Team
"""
import numpy as np
import pandas as pd
import dill  # Do not remove.
from sklearn.metrics import roc_auc_score
from sklearn.metrics import average_precision_score


class AnomalyScoreEvaluator(object):
    """
    AnomalyScoreEvaluator
    """

    def __init__(
        self,
        scoring_method="average",
        scoring_metric="anomaly_f1",
        scoring_topk_param=5,
        score_validation=0.5,
    ):
        """
        Constructor for anomaly_score_evaluator class.

        Parameters:
            scoring_method (string, optional): 'average' or 'topk'. Defaults to 'average'.
            scoring_metric (string, optional): 'roc_auc', 'anomaly_f1', 'anomaly_acc', 'pr_auc'. \
                Defaults to 'anomaly_f1'.
            scoring_topk_param (integer, optional): Positive, > 0 when score_method is 'top-k'. \
                Defaults to 5.
            score_validation (float, optional): Between 0 and 1. Defaults to 0.5.
        """
        self.scoring_method = scoring_method
        self.scoring_metric = scoring_metric
        self.scoring_topk_param = scoring_topk_param
        self.score_validation = score_validation

        # parameter generated by a score method
        self.ret_score = None
        self.best_f_measures = None
        self.best_thresholds = None
        self.best_anomaly_acc = None
        self.best_thresholds_anomaly_acc = None
        self.auc_roc = None
        self.auc_pr = None

    def score(self, anomaly_scores, test_data_labels):
        """
        Compute the score threshold based on the test data (labels from the test data if any) \
        and learnt model.

        Parameters:
            anomaly_scores (pandas.DataFrame/pandas.Series/numpy.ndarray, required): Anomaly scores.
            test_data_labels (pandas.DataFrame/pandas.Series/numpy.ndarray, required): Test data labels.

        Returns:
           float: Score of the anomaly model.
        """
        thresholds = None  # shared among all the calculations
        auc_roc = None
        auc_pr = None
        num_cols = 1
        self.best_thresholds = None

        if anomaly_scores is None:
            return 0

        # anomaly_scores are : attribute wise feature or per dataset
        if len(anomaly_scores.shape) > 1:
            num_cols = anomaly_scores.shape[1]
        num_rows = anomaly_scores.shape[0]

        # initialization
        best_f_measures = np.zeros(num_cols)
        best_thresholds = np.zeros(num_cols)
        best_anomaly_acc_measures = np.zeros(num_cols)
        best_anomaly_acc_thresholds = np.zeros(num_cols)
        auc_roc = np.zeros(num_cols)
        auc_pr = np.zeros(num_cols)

        for col in range(num_cols):
            if num_cols == 1:
                # Case when anomaly_scores is 1-D numpy array or pandas series
                scores = anomaly_scores[:]
            elif isinstance(anomaly_scores, pd.DataFrame):
                # Case when anomaly_scores is pandas dataframe
                scores = anomaly_scores.iloc[:, col]
            else:
                # Case when anomaly_scores is multi-dimensional numpy array
                scores = anomaly_scores[:, col]

            # label for the column under considerations
            tmp_test_data_labels = []

            if len(test_data_labels.shape) == 1:
                tmp_test_data_labels = np.array(list(test_data_labels))
            else:
                tmp_test_data_labels = test_data_labels[:, col]

            # prepare an intermediate columns, and then remove the na, infinite, +infinite
            result = pd.concat(
                [pd.DataFrame(scores), pd.DataFrame(tmp_test_data_labels)], axis=1
            )
            result = result.replace([np.inf, -np.inf], np.nan).dropna().values
            scores = result[:, 0]
            tmp_test_data_labels = result[:, 1]

            # if we do not have sufficient observation, we do not predict
            if (len(scores) * 1.0 / num_rows) < self.score_validation:
                continue

            if len(scores) == 0:
                continue

            # obtain min and max value
            minth = np.min(scores) - 0.001 * abs(np.max(scores) - np.min(scores))
            maxth = np.max(scores) + 0.001 * abs(np.max(scores) - np.min(scores))
            thresholds = []

            # to accommodate scalability
            if len(np.unique(scores)) < 1000:
                thresholds.append(minth)
                thresholds.extend(np.unique(scores))
                thresholds.append(maxth)
            else:
                stepsize = (maxth - minth) / 1000
                thresholds = list(np.arange(minth, maxth, stepsize))

            # for each threshold
            acc_norm = np.zeros(len(thresholds))
            acc_anom = np.zeros(len(thresholds))
            anomaly_acc = np.zeros(len(thresholds))

            for i, _ in enumerate(thresholds):
                threshold = thresholds[i]
                try:
                    acc = self.compute_accuracy_for_threshold(
                        scores, tmp_test_data_labels, threshold
                    )
                    acc_norm[i] = acc[0]
                    acc_anom[i] = acc[1]
                    anomaly_acc[i] = acc[2]
                except IndexError:
                    pass
                except TypeError:
                    pass

            f_measure = 2 * acc_norm * acc_anom / (acc_anom + acc_norm)
            best_tmp_f_measure = np.nanmax(f_measure)
            if not np.isnan(best_tmp_f_measure):
                best_result_index = (np.where(f_measure == best_tmp_f_measure))[0][0]
                best_f_measures[col] = best_tmp_f_measure
                best_thresholds[col] = thresholds[best_result_index]

            best_tmp_anomaly_acc_measure = np.nanmax(anomaly_acc)
            if not np.isnan(best_tmp_anomaly_acc_measure):
                best_tmp_anomaly_acc_measure_index = np.where(
                    anomaly_acc == best_tmp_anomaly_acc_measure
                )[0][0]
                best_anomaly_acc_measures[col] = best_tmp_anomaly_acc_measure
                best_anomaly_acc_thresholds[col] = thresholds[
                    best_tmp_anomaly_acc_measure_index
                ]

            try:
                auc_roc[col] = roc_auc_score(tmp_test_data_labels.astype(int), scores)
            except ValueError:
                auc_roc[col] = 1

            try:
                auc_pr[col] = average_precision_score(
                    tmp_test_data_labels.astype(int), scores
                )
            except ValueError:
                auc_pr[col] = 1

        self.best_f_measures = best_f_measures
        self.best_thresholds = best_thresholds
        self.auc_roc = auc_roc
        self.auc_pr = auc_pr
        self.best_anomaly_acc = best_anomaly_acc_measures
        self.best_thresholds_anomaly_acc = best_anomaly_acc_thresholds

        ret_score = 0

        if self.scoring_method == "average":
            if self.scoring_metric == "roc_auc":
                ret_score = np.nanmean(self.auc_roc)
            elif self.scoring_metric == "anomaly_f1":
                ret_score = np.nanmean(self.best_f_measures)
            elif self.scoring_metric == "anomaly_acc":
                ret_score = np.nanmean(self.best_anomaly_acc)
            elif self.scoring_metric == "pr_auc":
                ret_score = np.nanmean(self.auc_pr)
            else:
                pass
        elif self.scoring_method == "topk":
            if self.scoring_topk_param <= 0:
                raise Exception("scoring_topk_param is not > 1")
            if self.scoring_metric == "roc_auc":
                ret_score = self.get_average_return_score(
                    self.auc_roc, self.scoring_topk_param
                )
            elif self.scoring_metric == "anomaly_f1":
                ret_score = self.get_average_return_score(
                    self.best_f_measures, self.scoring_topk_param
                )
            elif self.scoring_metric == "anomaly_acc":
                ret_score = self.get_average_return_score(
                    self.best_anomaly_acc, self.scoring_topk_param
                )
            elif self.scoring_metric == "pr_auc":
                ret_score = self.get_average_return_score(
                    self.auc_pr, self.scoring_topk_param
                )
            else:
                pass
        else:
            pass

        # adding method for
        self.ret_score = ret_score
        return self.ret_score

    def get_best_thresholds(self):
        """
        Returns threshold based on scoring metric.

        Returns:
            float: Threshold.
        """
        if self.scoring_metric == "roc_auc":
            raise Exception("Not Applicable for roc_auc.")
        elif self.scoring_metric == "anomaly_f1":
            return self.best_thresholds
        elif self.scoring_metric == "anomaly_acc":
            return self.best_thresholds_anomaly_acc
        elif self.scoring_metric == "pr_auc":
            raise Exception("Not Applicable for pr_auc.")
        else:
            raise Exception("Unknown scoring metric.")

    def get_average_return_score(self, attribute_wise_score, topk_param):
        """
        Get average_return_score.

        Parameters:
            attribute_wise_score (numpy.ndarray, required): Attribute wise score.
            topk_param (integer, required): topk_param.
        
        Returns:
            ret_score (float): Score of the anomaly model.
        """
        tmp_f_mea = attribute_wise_score.copy()
        tmp_f_mea.sort()
        if len(tmp_f_mea) > topk_param:
            ret_score = np.nanmean(tmp_f_mea[len(tmp_f_mea) - topk_param :])
        else:
            ret_score = np.nanmean(tmp_f_mea)
        return ret_score

    def compute_accuracy_for_threshold(self, scores, test_data_label_col, threshold):
        """
        Computes accuracy for threshold.

        Parameters:
            scores (required): Anomaly scores.
            test_data_label_col (numpy.ndarray, required): Test data label column.
            threshold (float, required): Threshold.

        Returns:
            acc_norm
            acc_anom
            overall_acc
        """
        # This code assumes that there are only two classes: normal and anomalous.
        pred_pos_idx = np.where(scores >= threshold)
        pred_neg_idx = np.where(scores < threshold)
        true_pos_idx = np.where(
            test_data_label_col.astype(int) == 1
        )  # anomalous label is "1"
        true_neg_idx = np.where(test_data_label_col.astype(int) == 0)

        if len(true_pos_idx[0]) == 0:
            # as per Ide-san's code, when there are no anomalies as per the label,
            # the anomalous class accuracy is set to 1
            acc_anom = 1
        else:
            acc_anom = (1.0 * len(set(pred_pos_idx[0]) & set(true_pos_idx[0]))) / (
                1.0 * len(true_pos_idx[0])
            )

        if len(true_neg_idx[0]) == 0:
            acc_norm = 1
        else:
            acc_norm = (1.0 * len(set(pred_neg_idx[0]) & set(true_neg_idx[0]))) / (
                1.0 * len(true_neg_idx[0])
            )

        if len(true_pos_idx[0]) == 0:
            overall_acc = 1
        elif len(pred_pos_idx[0]) == 0:
            overall_acc = 0
        else:
            overall_acc = (1.0 * len(set(pred_pos_idx[0]) & set(true_pos_idx[0]))) / (
                1.0 * len(pred_pos_idx[0])
            )

        return acc_norm, acc_anom, overall_acc
